Feature engineering:

The main feature engineering that I did include encoding the team names, converting some of the data types, and adding a feature called 'score_diff'. 

I used the pandas one hot encoding functionality called 'get_dummies', rather than the sklearn one hot encoding functionality, because it interfaces better with pandas dataframes since it is native to pandas. 

At first I attempted to encode the pitchers, however, after google colab crashed, I realized that the pitcher stats were probably more important than the individual pitcher names. 

Models:

If I had enought memory, I would have liked to test the MultiOutputRegression model with LinearRegression to compare it to the RandomForestRegression and GradientBoostingRegression models. 

Also, I would have like to view some of the trees from the RandomForestRegression model. I think that the issues I encountered were caused by the multioutput nature of the model. 

Discussion of Results:

The RandomForestRegression model was able to predict both score1 and score2, and the mean squared error was 0.00004791. I attempted 10-fold cross validation of this model, but it crashed google colab multiple times.

The GradientBoostingRegression model was also able to make predictions, but its results are not as intuitive. In addition, it's mean squared error was quite large. One useful aspect of this model was the feature importance graph, which shows the 22 most important features. 

The MultiOutputRegression model was not able to train or make any predictions because it crashed google colab. 
